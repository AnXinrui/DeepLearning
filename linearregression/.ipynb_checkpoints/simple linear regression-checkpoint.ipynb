{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4d82e9-4e4e-4d7c-b8d6-38ff01c58f83",
   "metadata": {},
   "source": [
    "### Naive implementation of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68e41bf-b7ba-4e1b-a9bc-49a78c4b4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208c5f8-3e49-4dbe-8d67-c0be585aa47b",
   "metadata": {},
   "source": [
    "- synthetic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2fc266-ce52-47ac-9c47-1853297c5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, nums):\n",
    "    # 生成一个形状为 (nums, len(w)) 的张量 X，\n",
    "    # 每个元素都是从均值为 0 和标准差为 1 的正态分布中采样的随机数\n",
    "    X = torch.normal(0, 1, (nums, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "real_w = torch.tensor([4.0, 8])\n",
    "real_b = 13\n",
    "\n",
    "features, labels = synthetic_data(real_w, real_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d124de-f350-45e8-9140-3af1b08f9706",
   "metadata": {},
   "source": [
    "- Read data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b153f55-fe24-4438-8a6d-a41c76f5e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    nums = len(features)\n",
    "    # 随机读取样本\n",
    "    index = list(range(nums))\n",
    "    random.shuffle(index)\n",
    "    for i in range(0, nums, batch_size):\n",
    "        batch_index = torch.tensor(\n",
    "            index[i : min(i + batch_size, nums)]\n",
    "        )\n",
    "        yield features[batch_index], labels[batch_index]\n",
    "        \n",
    "# babatch_size = 10\n",
    "# for X, y in data_iter(batch_size, features, labels):\n",
    "#     print(X, '\\n', y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6faf8e-533d-40b5-b545-845917340e81",
   "metadata": {},
   "source": [
    "- Definition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a372ef9d-6d6b-41af-9113-6e61b64e45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8416073-b0f3-4245-9a9c-bed5525e977a",
   "metadata": {},
   "source": [
    "- Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15292723-09ef-4acb-8cd6-f2957a558fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835feed-3e7f-4ae1-92b3-31941c65a4d8",
   "metadata": {},
   "source": [
    "- Define optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91be84f-c49e-4378-b6c8-3d2a84728493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8a9ad-21af-428e-a749-a7201c2464d4",
   "metadata": {},
   "source": [
    "- trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c7d764-530a-4e50-84f5-904f197e70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化随机参数\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ce78da-0e26-41ad-8822-ddd712baf735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.320065\n",
      "epoch 2, loss 0.044475\n",
      "epoch 3, loss 0.000910\n"
     ]
    }
   ],
   "source": [
    "# 超参数\n",
    "batch_size = 10\n",
    "lr = 0.02\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y)\n",
    "        l.sum().backward();\n",
    "        sgd([w, b], lr, batch_size)\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        # 打印当前轮次和平均损失，损失值格式化为浮点数\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c61b70-4444-4b48-bb68-80c7af77d5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.9932],\n",
      "        [7.9757]], requires_grad=True)\n",
      "tensor([12.9666], requires_grad=True)\n",
      "w的估计误差: tensor([0.0068, 0.0243], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([0.0334], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)\n",
    "print(f'w的估计误差: {real_w - w.reshape(real_w.shape)}')\n",
    "print(f'b的估计误差: {real_b - b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
